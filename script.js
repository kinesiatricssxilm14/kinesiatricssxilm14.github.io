/**
 * å­¦æœ¯ä¸»é¡µ JavaScript åŠŸèƒ½
 * ä¸»è¦å®ç°ç§»åŠ¨ç«¯å¯¼èˆªèœå•çš„å±•å¼€å’Œæ”¶èµ·
 */

document.addEventListener('DOMContentLoaded', function() {
    // è·å–æ±‰å ¡èœå•æŒ‰é’®å’Œå¯¼èˆªé“¾æ¥å…ƒç´ 
    const hamburger = document.querySelector('.hamburger');
    const navLinks = document.querySelector('.nav-links');
    
    // ç‚¹å‡»æ±‰å ¡èœå•æŒ‰é’®æ—¶åˆ‡æ¢å¯¼èˆªèœå•çš„æ˜¾ç¤ºçŠ¶æ€
    hamburger.addEventListener('click', function() {
        navLinks.classList.toggle('active');
        
        // æ±‰å ¡èœå•æŒ‰é’®åŠ¨ç”»æ•ˆæœ
        const spans = hamburger.querySelectorAll('span');
        spans.forEach(span => {
            span.classList.toggle('active');
        });
    });
    
    // ç‚¹å‡»å¯¼èˆªé“¾æ¥åå…³é—­èœå•ï¼ˆç§»åŠ¨ç«¯ï¼‰
    const links = document.querySelectorAll('.nav-links a');
    links.forEach(link => {
        link.addEventListener('click', function() {
            if (window.innerWidth <= 768) {
                navLinks.classList.remove('active');
                
                // æ¢å¤æ±‰å ¡èœå•æŒ‰é’®çŠ¶æ€
                const spans = hamburger.querySelectorAll('span');
                spans.forEach(span => {
                    span.classList.remove('active');
                });
            }
        });
    });
    
    // æ·»åŠ å¹³æ»‘æ»šåŠ¨æ•ˆæœ
    links.forEach(link => {
        link.addEventListener('click', function(e) {
            e.preventDefault();
            
            const targetId = this.getAttribute('href');
            const targetElement = document.querySelector(targetId);
            
            if (targetElement) {
                const headerHeight = document.querySelector('header').offsetHeight;
                window.scrollTo({
                    top: targetElement.offsetTop - headerHeight - 20, // å‡å»å¯¼èˆªæ é«˜åº¦å’Œé¢å¤–é—´è·
                    behavior: 'smooth'
                });
            }
        });
    });
    
    // ç›‘å¬çª—å£å¤§å°å˜åŒ–ï¼Œåœ¨å¤§å±å¹•ä¸‹é‡ç½®å¯¼èˆªèœå•çŠ¶æ€
    window.addEventListener('resize', function() {
        if (window.innerWidth > 768) {
            navLinks.classList.remove('active');
            
            // æ¢å¤æ±‰å ¡èœå•æŒ‰é’®çŠ¶æ€
            const spans = hamburger.querySelectorAll('span');
            spans.forEach(span => {
                span.classList.remove('active');
            });
        }
    });
    
    // åˆ›å»ºåŠ¨æ€è£…é¥°èƒŒæ™¯
    createFloatingDecorations();
    
    // æ·»åŠ é¼ æ ‡äº¤äº’æ•ˆæœ
    addMouseInteraction();
});

// åˆ›å»ºæµ®åŠ¨è£…é¥°æ•ˆæœ
function createFloatingDecorations() {
    const decorationsContainer = document.createElement('div');
    decorationsContainer.className = 'floating-decorations';
    document.body.appendChild(decorationsContainer);
    
    // è£…é¥°å…ƒç´ ç±»å‹
    const elementTypes = ['circle', 'triangle', 'diamond', 'star'];
    
    // åˆ›å»ºå¤šä¸ªè£…é¥°å…ƒç´ 
    for (let i = 0; i < 12; i++) {
        createFloatingElement(decorationsContainer, elementTypes);
    }
    
    // å®šæœŸåˆ›å»ºæ–°çš„è£…é¥°å…ƒç´ 
    setInterval(() => {
        if (Math.random() < 0.4) { // 40%æ¦‚ç‡åˆ›å»ºæ–°å…ƒç´ 
            createFloatingElement(decorationsContainer, elementTypes);
        }
    }, 3000);
}

function createFloatingElement(container, types) {
    const element = document.createElement('div');
    const randomType = types[Math.floor(Math.random() * types.length)];
    element.className = `floating-element ${randomType}`;
    
    // éšæœºä½ç½®å’ŒåŠ¨ç”»æ—¶é•¿
    const leftPosition = Math.random() * 100;
    const animationDuration = 8 + Math.random() * 12; // 8-20ç§’
    const animationDelay = Math.random() * 3; // 0-3ç§’å»¶è¿Ÿ
    
    element.style.left = leftPosition + '%';
    element.style.animationDuration = animationDuration + 's';
    element.style.animationDelay = animationDelay + 's';
    
    container.appendChild(element);
    
    // æ·»åŠ ç‚¹å‡»æ•ˆæœ
    element.addEventListener('click', () => {
        element.style.transform += ' scale(1.5)';
        element.style.filter = 'brightness(1.5) saturate(1.5)';
        setTimeout(() => {
            element.style.transform = element.style.transform.replace(' scale(1.5)', '');
            element.style.filter = '';
        }, 300);
    });
    
    // åŠ¨ç”»ç»“æŸåç§»é™¤å…ƒç´ 
    setTimeout(() => {
        if (element.parentNode) {
            element.parentNode.removeChild(element);
        }
    }, (animationDuration + animationDelay) * 1000);
}

// æ·»åŠ é¼ æ ‡äº¤äº’æ•ˆæœ
function addMouseInteraction() {
    let mouseX = 0;
    let mouseY = 0;
    
    document.addEventListener('mousemove', (e) => {
        mouseX = e.clientX;
        mouseY = e.clientY;
        
        // è·å–æ‰€æœ‰è£…é¥°å…ƒç´ 
        const elements = document.querySelectorAll('.floating-element');
        elements.forEach(element => {
            const rect = element.getBoundingClientRect();
            const elementX = rect.left + rect.width / 2;
            const elementY = rect.top + rect.height / 2;
            
            // è®¡ç®—é¼ æ ‡ä¸è£…é¥°å…ƒç´ çš„è·ç¦»
            const distance = Math.sqrt(
                Math.pow(mouseX - elementX, 2) + Math.pow(mouseY - elementY, 2)
            );
            
            // å¦‚æœè·ç¦»å°äº120pxï¼Œäº§ç”Ÿå¸å¼•æ•ˆæœ
            if (distance < 120) {
                const angle = Math.atan2(mouseY - elementY, mouseX - elementX);
                const force = (120 - distance) / 120; // åŠ›åº¦ä¸è·ç¦»æˆåæ¯”
                const attractX = Math.cos(angle) * force * 30;
                const attractY = Math.sin(angle) * force * 30;
                
                element.classList.add('mouse-attract');
                element.style.transform = `translate(${attractX}px, ${attractY}px) rotate(${force * 90}deg) scale(${1 + force * 0.3})`;
            } else {
                element.classList.remove('mouse-attract');
                element.style.transform = '';
            }
        });
     });
}

// è®ºæ–‡è¯¦æƒ…æ•°æ®
const paperDetails = {
    'trae-agent': {
        title: 'Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling',
        authors: 'Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, Yun Lin, Yingfei Xiong, Chao Peng, Xia Liu, Trae Research Team',
        venue: 'Arxiv July, 2025',
        abstract: 'Software issue resolution is a critical challenge in software engineering and has garnered increasing attention in recent years. With the rapid advancement of large language models (LLMs), substantial progress has been made in addressing real-world software engineering tasks. Recent studies have introduced ensemble reasoning techniques to enhance the performance of LLM-based issue resolution. However, existing prompting-based methods still face limitations in effectively exploring large ensemble spaces and lack the capacity for repository-level understanding, both of which constrain their overall effectiveness. In this paper, we propose Trae Agent, the first agent-based ensemble reasoning approach for repository-level issue resolution. Trae Agent formulates our goal as an optimal solution search problem and addresses two key challenges, i.e., large ensemble spaces and repository-level understanding, through modular agents for generation, pruning, and selection. We conduct extensive experiments using three leading LLMs on the widely-adopted SWE-bench benchmark, comparing Trae Agent against four state-of-the-art ensemble reasoning techniques. Experimental results demonstrate that Trae Agent consistently achieves superior performance, with an average improvement of 10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of 75.20%. We are pleased to release Trae Agent as an open-source project to support the research community, with all resources available at https://github.com/bytedance/trae-agent.',
        image: 'paper_image/trae_agent.png',
        links: [
            { text: 'PDF', url: 'https://arxiv.org/pdf/2507.23370?' },
            { text: 'Code', url: 'https://github.com/bytedance/trae-agent' }
        ]
    },
    'repo2run': {
        title: 'Repo2Run: Automated Building Executable Environment for Code Repository at Scale',
        authors: 'Ruida Hu, Chao Peng*, Xinchen Wang, Junjielong Xu, Cuiyun Gao*',
        venue: '29th Annual Conference on Neural Information Processing Systems (NeurIPS 2025)',
        abstract: 'Scaling up executable code data is significant for improving language models\' software engineering capability. The intricate nature of the process makes it labor-intensive, time-consuming and expert-knowledge-dependent to build a large number of executable code repositories, limiting the scalability of existing work based on running tests. The primary bottleneck lies in the automated building of test environments for different repositories, which is an essential yet underexplored task. To mitigate the gap, we introduce Repo2Run, the first LLM-based agent aiming at automating the building of executable test environments for any repositories at scale. Specifically, given a code repository, Repo2Run iteratively builds the Docker image, runs unit tests based on the feedback of the building, and synthesizes the Dockerfile until the entire pipeline is executed successfully. The resulting Dockerfile can then be used to create Docker container environments for running code and tests. We created a benchmark containing 420 Python repositories with unit tests for evaluation. The results illustrate that Repo2Run achieves an 86.0% success rate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available at https://github.com/bytedance/Repo2Run.',
        image: 'paper_image/repo2run.png',
        links: [
            { text: 'PDF', url: 'https://arxiv.org/pdf/2502.13681' },
            { text: 'Code', url: 'https://github.com/bytedance/Repo2Run' }
        ]
    },
    'coderepoqa': {
        title: 'Understanding Large Language Model Performance in Software Engineering: A Large-scale Question Answering Benchmark',
        authors: 'Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, Cuiyun Gao*',
        venue: '48th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2025 short paper)',
        abstract: 'In this work, we introduce CodeRepoQA, a large-scale benchmark specifically designed for evaluating repository-level questionanswering capabilities in the field of software engineering. CodeRepoQA encompasses five programming languages and covers a wide range of scenarios, enabling comprehensive evaluation of languagemodels. To construct this dataset, we crawl data from 30 well-known repositories in GitHub, the largest platform for hosting and collaborating on code, and carefully filter the raw data. In total, CodeRepoQA is a multi-turn question-answering benchmark with 585,687 entries. It covers a diverse array of software engineering scenarios, with an average of 6.62 dialogue turns per entry. We evaluate ten popular large language models on our dataset and provide in-depth analysis. We find that LLMs still have limitations in question-answering capabilities in the field of software engineering, and medium-length contexts are more conducive to their performance. The entire benchmark and details are publicly available at https://github.com/kinesiatricssxilm14/CodeRepoQA.',
        image: 'paper_image/coderepoqa.png',
        links: [
            { text: 'PDF', url: 'https://dl.acm.org/doi/pdf/10.1145/3726302.3730262' },
            { text: 'Code', url: 'https://github.com/kinesiatricssxilm14/CodeRepoQA' }
        ]
    },
    'reposvul': {
        title: 'Reposvul: A repository-level high-quality vulnerability dataset',
        authors: 'Xinchen Wang#, Ruida Hu#, Cuiyun Gao*, Xin-Cheng Wen, Yujia Chen, Qing Liao',
        venue: '46th International Conference on Software Engineering: Companion Proceedings (ICSE 2024 Industry Challenge Track)',
        abstract: 'Open-Source Software (OSS) vulnerabilities bring great challenges to the software security and pose potential risks to our society. Enormous efforts have been devoted into automated vulnerability detection, among which deep learning (DL)-based approaches have proven to be the most effective. However, the current labeled data present the following limitations: (1) Tangled Patches: Developers may submit code changes unrelated to vulnerability fixes within patches, leading to tangled patches. (2) Lacking Inter-procedural Vulnerabilities: The existing vulnerability datasets typically contain function-level and file-level vulnerabilities, ignoring the relations between functions, thus rendering the approaches unable to detect the inter-procedural vulnerabilities. (3) Outdated Patches: The existing datasets usually contain outdated patches, which may bias the model during training.\nTo address the above limitations, in this paper, we propose an automated data collection framework and construct the first repository-level high-quality vulnerability dataset named ReposVul. The proposed framework mainly contains three modules: (1) A vulnerability untangling module, aiming at distinguishing vulnerability-fixing related code changes from tangled patches, in which the Large Language Models (LLMs) and static analysis tools are jointly employed. (2) A multi-granularity dependency extraction module, aiming at capturing the inter-procedural call relationships of vulnerabilities, in which we construct multiple-granularity information for each vulnerability patch, including repository-level, file-level, function-level, and line-level. (3) A trace-based filtering module, aiming at filtering the outdated patches, which leverages the file path trace-based filter and commit time trace-based filter to construct an up-to-date dataset.',
        image: 'paper_image/reposvul.png',
        links: [
            { text: 'PDF', url: 'https://arxiv.org/pdf/2401.13169' },
            { text: 'Code', url: 'https://github.com/Eshe0922/ReposVul' }
        ]
    },
    'codevisionary': {
        title: 'CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation',
        authors: 'Xinchen Wang, Pengfei Gao, Chao Peng, Ruida Hu, Cuiyun Gao',
        venue: '40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)',
        abstract: 'Large language models (LLMs) have demonstrated strong capabilities in code generation, underscoring the critical need for rigorous and comprehensive evaluation. Existing evaluation approaches fall into three categories, including human-centered, metric-based, and LLM-based. Considering that human-centered approaches are labour-intensive and metric-based ones overly rely on reference answers, LLM-based approaches are gaining increasing attention due to their stronger contextual understanding capabilities and superior efficiency. However, the performance of LLM-based approaches remains limited due to: (1) lack of multisource domain knowledge, and (2) insufficient comprehension of complex code.\nTo mitigate the limitations, we propose CodeVisionary, the first LLM-based agent framework for evaluating LLMs in code generation. CodeVisionary consists of two stages: (1) Multiscore knowledge analysis stage, which aims to gather multisource and comprehensive domain knowledge by formulating and executing a stepwise evaluation plan. (2) Negotiation-based scoring stage, which involves multiple judges engaging in discussions to better comprehend the complex code and reach a consensus on the evaluation score. Extensive experiments demonstrate that CodeVisionary achieves the best performance for evaluating LLMs in code generation, outperforming the best baseline methods with average improvements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau coefficients, respectively. Besides, CodeVisionary provides detailed evaluation reports, which assist developers in identifying shortcomings and making improvements. The resources of CodeVisionary are available at https://anonymous.4open.science/r/CodeVisionary.',
        image: 'paper_image/codevisionary.png',
        links: [
            { text: 'PDF', url: 'https://arxiv.org/pdf/2504.13472' },
            { text: 'Code', url: 'https://anonymous.4open.science/r/CodeVisionary' }
        ]
    },
    'repomastereval': {
        title: 'Repomastereval: Evaluating code completion via real-world repositories',
        authors: 'Qinyun Wu, Chao Peng, Pengfei Gao, Ruida Hu, Haoyu Gan, Bo Jiang, Jinhe Tang, Zhiwen Deng, Zhanming Guan, Cuiyun Gao, Xia Liu, Ping Yang',
        venue: '40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025 Industry Showcase)',
        abstract: 'With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical. However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model. By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block. These limitations makes the evaluation poorly align with the practical scenarios of code completion tools. In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories. Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites. To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score. Our empirical evaluation on 6 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report difference in model performance in real-world scenarios. The deployment of RepoMasterEval in a collaborated company for one month also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model\'s performance in practice. Based on our findings, we call for the software engineering community to build more LLM benchmarks tailored for code generation tools taking the practical and complex development environment into consideration.',
        image: 'paper_image/repomastereval.png',
        links: [
            { text: 'PDF', url: 'https://arxiv.org/pdf/2408.03519?' }
        ]
    },
    'aegis': {
        title: 'AEGIS: An agent-based framework for general bug reproduction from issue descriptions',
        authors: 'Xinchen Wang, Pengfei Gao, Xiangxin Meng, Chao Peng, Ruida Hu, Yun Lin, Cuiyun Gao',
        venue: '33rd ACM International Conference on the Foundations of Software Engineering (FSE 2025 Industry Track)',
        abstract: 'In software maintenance, bug reproduction is essential for effective fault localization and repair. Manually writing reproduction scripts is a time-consuming task with high requirements for developers. Hence, automation of bug reproduction has increasingly attracted attention from researchers and practitioners. However, the existing studies on bug reproduction are generally limited to specific bug types such as program crashes, and hard to be applied to general bug reproduction. In this paper, considering the superior performance of agent-based methods in code intelligence tasks, we focus on designing an agent-based framework for the task. Directly employing agents would lead to limited bug reproduction performance, due to entangled subtasks, lengthy retrieved context, and unregulated actions. To mitigate the challenges, we propose an Automated gEneral buG reproductIon Scripts generation framework, named AEGIS, which is the first agent-based framework for the task. AEGIS mainly contains two modules: (1) A concise context construction module, which aims to guide the code agent in extracting structured information from issue descriptions, identifying issue-related code with detailed explanations, and integrating these elements to construct the concise context; (2) A FSM-based multi-feedback optimization module to further regulate the behavior of the code agent within the finite state machine (FSM), ensuring a controlled and efficient script generation process based on multi-dimensional feedback. Extensive experiments on the public benchmark dataset show that AEGIS outperforms the state-of-the-art baseline by 23.0% in F->P metric. In addition, the bug reproduction scripts generated by AEGIS can improve the relative resolved rate of Agentless by 12.5%.',
        image: 'paper_image/aegis.png',
        links: [
            { text: 'PDF', url: 'https://dl.acm.org/doi/pdf/10.1145/3696630.3728557' }
        ]
    },
    'vuleval': {
        title: 'Vuleval: Towards repository-level evaluation of software vulnerability detection',
        authors: 'Xin-Cheng Wen, Xinchen Wang, Yujia Chen, Ruida Hu, David Lo, Cuiyun Gao',
        venue: 'Arxiv April, 2024',
        abstract: 'Deep Learning (DL)-based methods have proven to be effective for software vulnerability detection, with a potential for substantial productivity enhancements for detecting vulnerabilities. Current methods mainly focus on detecting single functions (i.e., intra-procedural vulnerabilities), ignoring the more complex inter-procedural vulnerability detection scenarios in practice. For example, developers routinely engage with program analysis to detect vulnerabilities that span multiple functions within repositories. In addition, the widely-used benchmark datasets generally contain only intra-procedural vulnerabilities, leaving the assessment of inter-procedural vulnerability detection capabilities unexplored.\nTo mitigate the issues, we propose a repository-level evaluation system, named \textbf{VulEval}, aiming at evaluating the detection performance of inter- and intra-procedural vulnerabilities simultaneously. Specifically, VulEval consists of three interconnected evaluation tasks: \textbf{(1) Function-Level Vulnerability Detection}, aiming at detecting intra-procedural vulnerability given a code snippet; \textbf{(2) Vulnerability-Related Dependency Prediction}, aiming at retrieving the most relevant dependencies from call graphs for providing developers with explanations about the vulnerabilities; and \textbf{(3) Repository-Level Vulnerability Detection}, aiming at detecting inter-procedural vulnerabilities by combining with the dependencies identified in the second task. VulEval also consists of a large-scale dataset, with a total of 4,196 CVE entries, 232,239 functions, and corresponding 4,699 repository-level source code in C/C++ programming languages. Our analysis highlights the current progress and future directions for software vulnerability detection.',
        image: 'paper_image/vuleval.png',
        links: [
            { text: 'PDF', url: 'https://arxiv.org/pdf/2404.15596' }
        ]
    },
    'pyconf': {
        title: 'Less is more? an empirical study on configuration issues in python pypi ecosystem',
        authors: 'Yun Peng, Ruida Hu, Ruoke Wang, Cuiyun Gao, Shuqing Li, Michael R Lyu',
        venue: '46th international conference on software engineering (ICSE 2024)',
        abstract: 'Python is widely used in the open-source community, largely owing to the extensive support from diverse third-party libraries within the PyPI ecosystem. Nevertheless, the utilization of third-party libraries can potentially lead to conflicts in dependencies, prompting researchers to develop dependency conflict detectors. Moreover, endeavors have been made to automatically infer dependencies. These approaches focus on version-level checks and inference, based on the assumption that configurations of libraries in the PyPI ecosystem are correct. However, our study reveals that this assumption is not universally valid, and relying solely on version-level checks proves inadequate in ensuring compatible run-time environments. In this paper, we conduct an empirical study to comprehensively study the configuration issues in the PyPI ecosystem. Specifically, we propose PyConf, a source-level detector, for detecting potential configuration issues. PyConf employs three distinct checks, targeting the setup, packing, and usage stages of libraries, respectively. To evaluate the effectiveness of the current automatic dependency inference approaches, we build a benchmark called VLibs, comprising library releases that pass all three checks of PyConf. We identify 15 kinds of configuration issues and find that 183,864 library releases suffer from potential configuration issues. Remarkably, 68% of these issues can only be detected via the source-level check. Our experiment results show that the most advanced automatic dependency inference approach, PyEGo, can successfully infer dependencies for only 65% of library releases. The primary failures stem from dependency conflicts and the absence of required libraries in the generated configurations. Based on the empirical results, we derive six findings and draw two implications for open-source developers and future research in automatic dependency inference.',
        image: 'paper_image/pyconf.png',
        links: [
            { text: 'PDF', url: 'https://arxiv.org/pdf/2310.12598' },
            { text: 'Code', url: 'https://github.com/JohnnyPeng18/PyConf' }
        ]
    }
    // å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šè®ºæ–‡çš„è¯¦æƒ…
};

// æ‰“å¼€è®ºæ–‡è¯¦æƒ…æ¨¡æ€æ¡†
function openPaperModal(paperId) {
    const paper = paperDetails[paperId];
    if (!paper) return;
    
    const modalContent = document.getElementById('modalContent');
    modalContent.innerHTML = `
        <img src="${paper.image}" alt="${paper.title}" class="modal-image">
        <h2 style="color: var(--primary-color); margin-bottom: 15px;">${paper.title}</h2>
        <p style="font-style: italic; color: #666; margin-bottom: 10px;"><strong>Authors:</strong> ${paper.authors}</p>
        <p style="font-weight: 500; color: #555; margin-bottom: 20px;"><strong>Venue:</strong> ${paper.venue}</p>
        <h3 style="color: var(--primary-color); margin-bottom: 10px;">Abstract</h3>
        <p style="line-height: 1.6; margin-bottom: 20px;">${paper.abstract}</p>
        <div style="display: flex; gap: 10px;">
            ${paper.links.map(link => `<a href="${link.url}" class="btn btn-small" target="_blank">${link.text}</a>`).join('')}
        </div>
    `;
    
    document.getElementById('paperModal').style.display = 'block';
    document.body.style.overflow = 'hidden'; // é˜²æ­¢èƒŒæ™¯æ»šåŠ¨
}

// å…³é—­è®ºæ–‡è¯¦æƒ…æ¨¡æ€æ¡†
function closePaperModal() {
    document.getElementById('paperModal').style.display = 'none';
    document.body.style.overflow = 'auto'; // æ¢å¤æ»šåŠ¨
}

// ç‚¹å‡»æ¨¡æ€æ¡†å¤–éƒ¨å…³é—­
window.onclick = function(event) {
    const modal = document.getElementById('paperModal');
    if (event.target === modal) {
        closePaperModal();
    }
}

// ===== è®¿é—®ç»Ÿè®¡åŠŸèƒ½ =====

// è®¿é—®ç»Ÿè®¡æ•°æ®å­˜å‚¨
const visitStats = {
    totalVisits: 0,
    todayVisits: 0,
    locations: {},
    currentLocation: 'è·å–ä¸­...',
    lastVisitDate: null
};

// CountAPIé…ç½®
const COUNTAPI_CONFIG = {
    namespace: 'kinesiatricssxilm14-github-io',
    key: 'total-visits',
    todayKey: 'today-visits-' + new Date().toISOString().split('T')[0], // æ¯æ—¥å”¯ä¸€key
    locationPrefix: 'location-'
};

// åˆå§‹åŒ–è®¿é—®ç»Ÿè®¡
function initVisitStats() {
    // è·å–ç”¨æˆ·ä½ç½®ä¿¡æ¯
    getUserLocation();
    
    // æ›´æ–°å…¨çƒè®¿é—®è®¡æ•°
    updateGlobalVisitCount();
    
    // æ›´æ–°ä»Šæ—¥è®¿é—®è®¡æ•°
    updateTodayVisitCount();
    
    // åŠ è½½åœ°åŒºç»Ÿè®¡æ•°æ®
    loadLocationStats();
}

// æ›´æ–°å…¨çƒè®¿é—®è®¡æ•°
async function updateGlobalVisitCount() {
    try {
        console.log('ğŸŒ æ­£åœ¨è·å–å…¨çƒè®¿é—®ç»Ÿè®¡...');
        const url = `https://api.countapi.xyz/hit/${COUNTAPI_CONFIG.namespace}/${COUNTAPI_CONFIG.key}`;
        console.log('API URL:', url);
        
        const response = await fetch(url);
        const data = await response.json();
        
        console.log('âœ… å…¨çƒè®¿é—®ç»Ÿè®¡å“åº”:', data);
        visitStats.totalVisits = data.value || 0;
        updateStatsDisplay();
    } catch (error) {
        console.error('âŒ è·å–å…¨çƒè®¿é—®ç»Ÿè®¡å¤±è´¥:', error);
        // é™çº§åˆ°æœ¬åœ°å­˜å‚¨
        loadLocalVisitData();
    }
}

// æ›´æ–°ä»Šæ—¥è®¿é—®è®¡æ•°
async function updateTodayVisitCount() {
    try {
        console.log('ğŸ“… æ­£åœ¨è·å–ä»Šæ—¥è®¿é—®ç»Ÿè®¡...');
        const url = `https://api.countapi.xyz/hit/${COUNTAPI_CONFIG.namespace}/${COUNTAPI_CONFIG.todayKey}`;
        console.log('Today API URL:', url);
        
        const response = await fetch(url);
        const data = await response.json();
        
        console.log('âœ… ä»Šæ—¥è®¿é—®ç»Ÿè®¡å“åº”:', data);
        visitStats.todayVisits = data.value || 0;
        updateStatsDisplay();
    } catch (error) {
        console.error('âŒ è·å–ä»Šæ—¥è®¿é—®ç»Ÿè®¡å¤±è´¥:', error);
    }
}

// æ›´æ–°åœ°åŒºè®¿é—®ç»Ÿè®¡
async function updateLocationVisitCount(location) {
    if (!location) return;
    
    try {
        console.log('ğŸŒ æ­£åœ¨æ›´æ–°åœ°åŒºç»Ÿè®¡:', location);
        const locationKey = COUNTAPI_CONFIG.locationPrefix + encodeURIComponent(location);
        const url = `https://api.countapi.xyz/hit/${COUNTAPI_CONFIG.namespace}/${locationKey}`;
        console.log('Location API URL:', url);
        
        const response = await fetch(url);
        const data = await response.json();
        
        console.log('âœ… åœ°åŒºç»Ÿè®¡å“åº”:', data);
        visitStats.locations[location] = data.value || 1;
        updateLocationList();
    } catch (error) {
        console.error('âŒ æ›´æ–°åœ°åŒºç»Ÿè®¡å¤±è´¥:', error);
        // é™çº§åˆ°æœ¬åœ°å­˜å‚¨
        visitStats.locations[location] = (visitStats.locations[location] || 0) + 1;
        saveLocalLocationData();
        updateLocationList();
    }
}

// åŠ è½½åœ°åŒºç»Ÿè®¡æ•°æ®ï¼ˆè·å–å‰5ä¸ªåœ°åŒºï¼‰
async function loadLocationStats() {
    try {
        // ä»æœ¬åœ°å­˜å‚¨è·å–å·²çŸ¥åœ°åŒºåˆ—è¡¨
        const knownLocations = getKnownLocations();
        
        for (const location of knownLocations.slice(0, 10)) { // é™åˆ¶æŸ¥è¯¢æ•°é‡
            const locationKey = COUNTAPI_CONFIG.locationPrefix + encodeURIComponent(location);
            try {
                const response = await fetch(`https://api.countapi.xyz/get/${COUNTAPI_CONFIG.namespace}/${locationKey}`);
                const data = await response.json();
                if (data.value > 0) {
                    visitStats.locations[location] = data.value;
                }
            } catch (error) {
                console.log(`è·å–åœ°åŒº ${location} ç»Ÿè®¡å¤±è´¥:`, error);
            }
        }
        updateLocationList();
    } catch (error) {
        console.log('åŠ è½½åœ°åŒºç»Ÿè®¡å¤±è´¥:', error);
    }
}

// è·å–å·²çŸ¥åœ°åŒºåˆ—è¡¨
function getKnownLocations() {
    const saved = localStorage.getItem('knownLocations');
    return saved ? JSON.parse(saved) : [];
}

// ä¿å­˜å·²çŸ¥åœ°åŒº
function saveKnownLocation(location) {
    const known = getKnownLocations();
    if (!known.includes(location)) {
        known.push(location);
        localStorage.setItem('knownLocations', JSON.stringify(known));
    }
}

// é™çº§æ–¹æ¡ˆï¼šæœ¬åœ°å­˜å‚¨
function loadLocalVisitData() {
    const savedData = localStorage.getItem('visitStatsData');
    if (savedData) {
        const data = JSON.parse(savedData);
        visitStats.totalVisits = data.totalVisits || 0;
        visitStats.todayVisits = data.todayVisits || 0;
        visitStats.locations = data.locations || {};
        visitStats.lastVisitDate = data.lastVisitDate;
    }
    
    // æœ¬åœ°è®¡æ•°é€»è¾‘
    const today = new Date().toDateString();
    visitStats.totalVisits++;
    
    if (visitStats.lastVisitDate !== today) {
        visitStats.todayVisits = 1;
        visitStats.lastVisitDate = today;
    } else {
        visitStats.todayVisits++;
    }
    
    saveLocalVisitData();
    updateStatsDisplay();
}

// ä¿å­˜æœ¬åœ°è®¿é—®æ•°æ®
function saveLocalVisitData() {
    localStorage.setItem('visitStatsData', JSON.stringify(visitStats));
}

// ä¿å­˜æœ¬åœ°åœ°åŒºæ•°æ®
function saveLocalLocationData() {
    localStorage.setItem('locationStatsData', JSON.stringify(visitStats.locations));
}

// è·å–ç”¨æˆ·ä½ç½®ä¿¡æ¯
function getUserLocation() {
    // ä½¿ç”¨å…è´¹çš„IPåœ°ç†ä½ç½®API
    fetch('https://ipapi.co/json/')
        .then(response => response.json())
        .then(data => {
            if (data.city && data.country_name) {
                const location = `${data.city}, ${data.country_name}`;
                visitStats.currentLocation = location;
                
                // ä¿å­˜å·²çŸ¥åœ°åŒº
                saveKnownLocation(location);
                
                // æ›´æ–°åœ°åŒºè®¿é—®ç»Ÿè®¡ï¼ˆä½¿ç”¨å…¨çƒAPIï¼‰
                updateLocationVisitCount(location);
                
                // æ›´æ–°æ˜¾ç¤º
                updateStatsDisplay();
            }
        })
        .catch(error => {
            console.log('è·å–ä½ç½®ä¿¡æ¯å¤±è´¥:', error);
            visitStats.currentLocation = 'æœªçŸ¥ä½ç½®';
            updateStatsDisplay();
        });
}

// æ›´æ–°ç»Ÿè®¡æ˜¾ç¤º
function updateStatsDisplay() {
    // æ›´æ–°æ€»è®¿é—®é‡
    const totalElement = document.getElementById('totalVisits');
    if (totalElement) {
        totalElement.textContent = visitStats.totalVisits.toLocaleString();
    }
    
    // æ›´æ–°ä»Šæ—¥è®¿é—®
    const todayElement = document.getElementById('todayVisits');
    if (todayElement) {
        todayElement.textContent = visitStats.todayVisits.toLocaleString();
    }
    
    // æ›´æ–°å½“å‰ä½ç½®
    const locationElement = document.getElementById('currentLocation');
    if (locationElement) {
        locationElement.textContent = visitStats.currentLocation;
    }
    
    // æ›´æ–°è®¿é—®åœ°åŒºåˆ—è¡¨
    updateLocationList();
}

// æ›´æ–°åœ°åŒºåˆ—è¡¨æ˜¾ç¤º
function updateLocationList() {
    const locationListElement = document.getElementById('locationList');
    if (!locationListElement) return;
    
    // æŒ‰è®¿é—®æ¬¡æ•°æ’åº
    const sortedLocations = Object.entries(visitStats.locations)
        .sort(([,a], [,b]) => b - a)
        .slice(0, 5); // åªæ˜¾ç¤ºå‰5ä¸ªåœ°åŒº
    
    if (sortedLocations.length === 0) {
        locationListElement.innerHTML = '<div class="loading">æš‚æ— æ•°æ®</div>';
        return;
    }
    
    const locationHTML = sortedLocations.map(([location, count]) => `
        <div class="location-item">
            <span class="location-name">${location}</span>
            <span class="location-count">${count}</span>
        </div>
    `).join('');
    
    locationListElement.innerHTML = locationHTML;
}

// åˆ‡æ¢è®¿é—®ç»Ÿè®¡é¢æ¿æ˜¾ç¤º
function toggleVisitStats() {
    const panel = document.getElementById('visitStatsPanel');
    if (panel) {
        panel.classList.toggle('active');
    }
}

// æ£€æŸ¥æ˜¯å¦ä¸ºç®¡ç†å‘˜ï¼ˆç®€å•çš„å¯†ç éªŒè¯ï¼‰
function isAdmin() {
    const adminPassword = localStorage.getItem('adminPassword');
    return adminPassword === 'admin123'; // ç®€å•çš„å¯†ç éªŒè¯
}

// ç®¡ç†å‘˜ç™»å½•
function adminLogin() {
    const password = prompt('è¯·è¾“å…¥ç®¡ç†å‘˜å¯†ç :');
    if (password === 'admin123') {
        localStorage.setItem('adminPassword', password);
        alert('ç®¡ç†å‘˜ç™»å½•æˆåŠŸï¼');
        updateStatsVisibility();
        return true;
    } else if (password !== null) {
        alert('å¯†ç é”™è¯¯ï¼');
    }
    return false;
}

// ç®¡ç†å‘˜ç™»å‡º
function adminLogout() {
    localStorage.removeItem('adminPassword');
    updateStatsVisibility();
    alert('å·²é€€å‡ºç®¡ç†å‘˜æ¨¡å¼');
}

// æ›´æ–°ç»Ÿè®¡ç»„ä»¶çš„å¯è§æ€§
function updateStatsVisibility() {
    const statsContainer = document.getElementById('visitStats');
    if (!statsContainer) return;
    
    if (isAdmin()) {
        statsContainer.style.display = 'block';
    } else {
        statsContainer.style.display = 'none';
    }
}

// é¡µé¢åŠ è½½å®Œæˆååˆå§‹åŒ–
document.addEventListener('DOMContentLoaded', function() {
    // å»¶è¿Ÿåˆå§‹åŒ–è®¿é—®ç»Ÿè®¡ï¼Œé¿å…å½±å“é¡µé¢åŠ è½½é€Ÿåº¦
    setTimeout(() => {
        initVisitStats();
        updateStatsVisibility();
    }, 1000);
    
    // æ·»åŠ ç®¡ç†å‘˜å¿«æ·é”® (å¤šç§å…¼å®¹æ–¹å¼)
    document.addEventListener('keydown', function(e) {
        // æ–¹å¼1: Ctrl+Shift+A
        if (e.ctrlKey && e.shiftKey && (e.key === 'A' || e.keyCode === 65)) {
            e.preventDefault();
            if (isAdmin()) {
                adminLogout();
            } else {
                adminLogin();
            }
        }
        // æ–¹å¼2: Ctrl+Alt+S (å¤‡ç”¨å¿«æ·é”®)
        else if (e.ctrlKey && e.altKey && (e.key === 'S' || e.keyCode === 83)) {
            e.preventDefault();
            if (isAdmin()) {
                adminLogout();
            } else {
                adminLogin();
            }
        }
    });
    
    // æ·»åŠ éšè—çš„ç®¡ç†å‘˜å…¥å£ - è¿ç»­ç‚¹å‡»é¡µè„š5æ¬¡
    let footerClickCount = 0;
    let footerClickTimer = null;
    const footer = document.querySelector('footer');
    if (footer) {
        footer.addEventListener('click', function() {
            footerClickCount++;
            
            // æ¸…é™¤ä¹‹å‰çš„è®¡æ—¶å™¨
            if (footerClickTimer) {
                clearTimeout(footerClickTimer);
            }
            
            // å¦‚æœ5ç§’å†…ç‚¹å‡»5æ¬¡ï¼Œè§¦å‘ç®¡ç†å‘˜ç™»å½•
            if (footerClickCount >= 5) {
                footerClickCount = 0;
                if (isAdmin()) {
                    adminLogout();
                } else {
                    adminLogin();
                }
            } else {
                // 5ç§’åé‡ç½®è®¡æ•°
                footerClickTimer = setTimeout(() => {
                    footerClickCount = 0;
                }, 5000);
            }
        });
    }
});